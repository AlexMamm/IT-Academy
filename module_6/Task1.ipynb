{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74cebdbd",
   "metadata": {},
   "source": [
    "## Алгоритмы мультиклассовой классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0adcbeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data_dir = \"bbc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c5e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "regex = re.compile(\"[A-Za-z]+\")\n",
    "mystopwords = stopwords.words('english')\n",
    "\n",
    "def get_dataset(path):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            file_path = os.path.join(root, name)\n",
    "            category = file_path.split('\\\\')[-2]\n",
    "            text = \" \".join(open(file_path, encoding='utf-8', errors = 'ignore').read().splitlines())\n",
    "            data.append((category,text))\n",
    "            \n",
    "    df = pd.DataFrame(data, columns=[\"category\",\"text\"])\n",
    "    shuffle_df = df.sample(frac = 1, random_state = 42, ignore_index = True)\n",
    "    return shuffle_df\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    return \" \".join(regex.findall(text))\n",
    "\n",
    "def lemmatize(text, mystopwords = mystopwords):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(token) for token in text.split() if not token in mystopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce33fa",
   "metadata": {},
   "source": [
    "### Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e947aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset(data_dir)\n",
    "df.text = df.text.apply(words_only)\n",
    "df.text = df.text.apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ef9c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>UK house price dip November UK house price dip...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>LSE set date takeover deal The London Stock Ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>Harinordoquy suffers France axe Number eight I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Barclays share merger talk Shares UK banking g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics</td>\n",
       "      <td>Campaign cold call questioned Labour Conservat...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text  labels\n",
       "0  business  UK house price dip November UK house price dip...       0\n",
       "1  business  LSE set date takeover deal The London Stock Ex...       0\n",
       "2     sport  Harinordoquy suffers France axe Number eight I...       1\n",
       "3  business  Barclays share merger talk Shares UK banking g...       0\n",
       "4  politics  Campaign cold call questioned Labour Conservat...       2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df.category.unique()\n",
    "num_labels = len(labels)\n",
    "id2label = {i:l for i,l in enumerate(labels)}\n",
    "label2id = {l:i for i,l in enumerate(labels)}\n",
    "df[\"labels\"] = df.category.map(lambda x: label2id[x.strip()])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7ad6a",
   "metadata": {},
   "source": [
    "### Разбиение выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd965591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape:  (1557,) (1557,)\n",
      "Valid Shape:  (334,) (334,)\n",
      "Test Shape:  (334,) (334,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(df.text, df.labels, stratify = df.labels, train_size=0.7)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_rem, y_rem, stratify = y_rem, test_size=0.5)\n",
    "\n",
    "print('Train Shape: ', X_train.shape, y_train.shape)\n",
    "print('Valid Shape: ', X_valid.shape, y_valid.shape)\n",
    "print('Test Shape: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18af560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features = 5000)\n",
    "vectorizer.fit(df.text)\n",
    "\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_valid = vectorizer.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef5e62",
   "metadata": {},
   "source": [
    "### Перебор гиперпараметров алгоритмов многоклассовой классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5918d305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Logistic Regression - Params: {'penalty': 'l2', 'tol': 0.01}, Score: 0.974\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Random Forest - Params: {'max_depth': 9, 'n_estimators': 20}, Score: 0.92\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "XGBoost - Params: {'eta': 0.5, 'max_depth': 5, 'n_estimators': 20}, Score: 0.951\n",
      "Model: Logistic Regression, Accuracy: 0.982, F1: 0.982\n",
      "Model: Random Forest, Accuracy: 0.925, F1: 0.925\n",
      "Model: XGB, Accuracy: 0.952, F1: 0.952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr = LogisticRegression()\n",
    "forest = RandomForestClassifier() \n",
    "xgb = XGBClassifier()\n",
    "\n",
    "lr_params = {'penalty': ['l1','l2'], 'tol': [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1]}\n",
    "forest_params = {'n_estimators': range(10,30,5), 'max_depth': range(5,10)} \n",
    "xgb_params = {'n_estimators': range(10,30,10), 'eta': [0.1, 0.5, 0.9], 'max_depth': range(5,8)}\n",
    "\n",
    "lr_grid = GridSearchCV(lr, lr_params, cv=5, verbose=True, n_jobs=-1)\n",
    "forest_grid = GridSearchCV(forest, forest_params,cv=5, verbose=True, n_jobs=-1)\n",
    "xgb_grid = GridSearchCV(xgb, xgb_params, cv=5, verbose=True, n_jobs=-1)\n",
    "\n",
    "lr_grid.fit(X_train, y_train)\n",
    "print(\"Logistic Regression - Params: {}, Score: {}\".format(lr_grid.best_params_, round(lr_grid.best_score_,3)))\n",
    "forest_grid.fit(X_train, y_train)\n",
    "print(\"Random Forest - Params: {}, Score: {}\".format(forest_grid.best_params_, round(forest_grid.best_score_,3)))\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "print(\"XGBoost - Params: {}, Score: {}\".format(xgb_grid.best_params_, round(xgb_grid.best_score_,3)))\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(penalty=lr_grid.best_params_['penalty'], tol=lr_grid.best_params_['tol']),\n",
    "    RandomForestClassifier(n_estimators=forest_grid.best_params_['n_estimators'], max_depth=forest_grid.best_params_['max_depth']),\n",
    "    XGBClassifier(eta=xgb_grid.best_params_['eta'], max_depth=xgb_grid.best_params_['max_depth'], n_estimators=xgb_grid.best_params_['n_estimators']) \n",
    "]\n",
    "names  = [\"Logistic Regression\",\"Random Forest\",\"XGB\"]\n",
    "for model,name in zip(models, names):\n",
    "    model.fit(X_train,y_train)\n",
    "        \n",
    "    predicted = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, predicted)\n",
    "    f1 = f1_score(y_valid, predicted, average = 'micro')\n",
    "\n",
    "    print(\"Model: {}, Accuracy: {}, F1: {}\".format(name, round(accuracy,3),round(f1,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c857814a",
   "metadata": {},
   "source": [
    "### Проверка на валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0bb448e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression, Accuracy: 0.964, F1: 0.964\n",
      "Model: Random Forest, Accuracy: 0.91, F1: 0.91\n",
      "Model: XGB, Accuracy: 0.925, F1: 0.925\n"
     ]
    }
   ],
   "source": [
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "names  = [\"Logistic Regression\",\"Random Forest\",\"XGB\"]\n",
    "for model,name in zip(models, names):\n",
    "    model.fit(X_train,y_train)\n",
    "        \n",
    "    predicted = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predicted)\n",
    "    f1 = f1_score(y_test, predicted, average = 'micro')\n",
    "\n",
    "    print(\"Model: {}, Accuracy: {}, F1: {}\".format(name, round(accuracy,3),round(f1,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba29f75",
   "metadata": {},
   "source": [
    "## DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bfbc56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "torch.cuda.is_available = lambda : False\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d249894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb58acbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape:  (1557,) (1557,)\n",
      "Valid Shape:  (334,) (334,)\n",
      "Test Shape:  (334,) (334,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(df.text, df.labels, stratify = df.labels, train_size=0.7)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_rem, y_rem, stratify = y_rem, test_size=0.5)\n",
    "\n",
    "print('Train Shape: ', X_train.shape, y_train.shape)\n",
    "print('Valid Shape: ', X_valid.shape, y_valid.shape)\n",
    "print('Test Shape: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96449cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values, valid_values, test_values = list(X_train.values), list(X_valid.values), list(X_test.values)\n",
    "train_labels, valid_labels, test_labels = list(y_train.values), list(y_valid.values), list(y_test.values)\n",
    "\n",
    "train_encodings = tokenizer(train_values, truncation=True, padding=True)\n",
    "val_encodings  = tokenizer(valid_values, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_values, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "951a9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = MyDataset(train_encodings, train_labels)\n",
    "val_dataset = MyDataset(val_encodings, valid_labels)\n",
    "test_dataset = MyDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e69bdc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='outputs/TBERT/',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=3,\n",
    "    auto_find_batch_size = True,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',\n",
    "    logging_dir='outputs/logs/',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics= compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d87a16cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1557\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 585\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='585' max='585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [585/585 1:20:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>0.187078</td>\n",
       "      <td>0.952096</td>\n",
       "      <td>0.950956</td>\n",
       "      <td>0.957567</td>\n",
       "      <td>0.948924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.149560</td>\n",
       "      <td>0.970060</td>\n",
       "      <td>0.970024</td>\n",
       "      <td>0.970505</td>\n",
       "      <td>0.971508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.133563</td>\n",
       "      <td>0.973054</td>\n",
       "      <td>0.973426</td>\n",
       "      <td>0.973352</td>\n",
       "      <td>0.973870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 334\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/TBERT/checkpoint-195\n",
      "Configuration saved in outputs/TBERT/checkpoint-195\\config.json\n",
      "Model weights saved in outputs/TBERT/checkpoint-195\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 334\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/TBERT/checkpoint-390\n",
      "Configuration saved in outputs/TBERT/checkpoint-390\\config.json\n",
      "Model weights saved in outputs/TBERT/checkpoint-390\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 334\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/TBERT/checkpoint-585\n",
      "Configuration saved in outputs/TBERT/checkpoint-585\\config.json\n",
      "Model weights saved in outputs/TBERT/checkpoint-585\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/TBERT/checkpoint-585 (score: 0.13356250524520874).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=585, training_loss=0.21884407808638026, metrics={'train_runtime': 4827.5851, 'train_samples_per_second': 0.968, 'train_steps_per_second': 0.121, 'total_flos': 618788322984960.0, 'train_loss': 0.21884407808638026, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e93b05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1557\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='279' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [195/195 14:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 334\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 334\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_Accuracy</th>\n",
       "      <th>eval_F1</th>\n",
       "      <th>eval_Precision</th>\n",
       "      <th>eval_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.011190</td>\n",
       "      <td>0.996146</td>\n",
       "      <td>0.996167</td>\n",
       "      <td>0.996377</td>\n",
       "      <td>0.995972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>0.133563</td>\n",
       "      <td>0.973054</td>\n",
       "      <td>0.973426</td>\n",
       "      <td>0.973352</td>\n",
       "      <td>0.973870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.071997</td>\n",
       "      <td>0.985030</td>\n",
       "      <td>0.985583</td>\n",
       "      <td>0.985209</td>\n",
       "      <td>0.986385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       eval_loss  eval_Accuracy   eval_F1  eval_Precision  eval_Recall\n",
       "train   0.011190       0.996146  0.996167        0.996377     0.995972\n",
       "val     0.133563       0.973054  0.973426        0.973352     0.973870\n",
       "test    0.071997       0.985030  0.985583        0.985209     0.986385"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = [trainer.evaluate(eval_dataset=data) for data in [train_dataset, val_dataset, test_dataset]]\n",
    "pd.DataFrame(q, index=[\"train\",\"val\",\"test\"]).iloc[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12aea03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cpu\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    return probs, probs.argmax(),model.config.id2label[probs.argmax().item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73df713",
   "metadata": {},
   "source": [
    "### Сегодняшняя  [новость](https://www.bbc.com/news/business-63128436) из категории business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09b1a295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9614, 0.0122, 0.0122, 0.0090, 0.0052]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor(0),\n",
       " 'business')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The pound has risen to its highest level for two weeks on hopes Kwasi Kwarteng will bring forward details of how he will cut debt.\"\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cae8e258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to outputs/TBERT\n",
      "Configuration saved in outputs/TBERT\\config.json\n",
      "Model weights saved in outputs/TBERT\\pytorch_model.bin\n",
      "tokenizer config file saved in outputs/TBERT\\tokenizer_config.json\n",
      "Special tokens file saved in outputs/TBERT\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('outputs/TBERT\\\\tokenizer_config.json',\n",
       " 'outputs/TBERT\\\\special_tokens_map.json',\n",
       " 'outputs/TBERT\\\\vocab.txt',\n",
       " 'outputs/TBERT\\\\added_tokens.json',\n",
       " 'outputs/TBERT\\\\tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"outputs/TBERT\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bb8e988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file outputs/TBERT\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"business\",\n",
      "    \"1\": \"sport\",\n",
      "    \"2\": \"politics\",\n",
      "    \"3\": \"entertainment\",\n",
      "    \"4\": \"tech\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"business\": 0,\n",
      "    \"entertainment\": 3,\n",
      "    \"politics\": 2,\n",
      "    \"sport\": 1,\n",
      "    \"tech\": 4\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file outputs/TBERT\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at outputs/TBERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc69fe63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'business', 'score': 0.961361825466156}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"The pound has risen to its highest level for two weeks on hopes Kwasi Kwarteng will bring forward details of how he will cut debt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0583926",
   "metadata": {},
   "source": [
    "По качеству на валидационной выборке видно, что DistilBert с оценкой 0.985 сработал лучше, чем логистическая регрессия с оценкой 0.964"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
