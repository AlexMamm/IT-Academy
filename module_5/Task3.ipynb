{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YbzZhFZOfWvE"
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.cuda.is_available = lambda : False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f2eh0h3VvKaz"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class LanguageVocabulary(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            # Если такое уже слово есть просто добавляем 1 что добавилось одно слово\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FB4bJicWvL80"
   },
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5n_QzLiSvNaf"
   },
   "outputs": [],
   "source": [
    "def read_languages(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    lines = open('deu.txt', encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = LanguageVocabulary(lang2)\n",
    "        output_lang = LanguageVocabulary(lang1)\n",
    "    else:\n",
    "        input_lang = LanguageVocabulary(lang1)\n",
    "        output_lang = LanguageVocabulary(lang2)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ctj7mvnYvW0E"
   },
   "outputs": [],
   "source": [
    "def prepare_data(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_languages(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7MHtch2vXPz",
    "outputId": "c44f7e76-7130-4d54-fc35-d4acf25ba667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 255817 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "deu 36316\n",
      "eng 16400\n",
      "['ich habe einen fruheren flug genommen .', 'i caught an earlier flight .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data('eng', 'deu', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wFhw0wYJvbMQ"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tBn6mty0vejW"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0])) \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cKZn8Y3xvf0u"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GxUl-8r6viY5"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach() \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wyVX68Oevj5_"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / percent\n",
    "    rs = es - s\n",
    "    return '%s (- eta: %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3D7V_kEBvlEZ"
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.05):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  \n",
    "    plot_loss_total = 0  \n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_iters):\n",
    "        training_pair = training_pairs[epoch - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_iters),\n",
    "                                         epoch, epoch / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AnSxWnG_vmpq"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SKom-y6Zvnzv"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "            encoder_outputs[i] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = [] # Наши деокдированные слова\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kFwMn4ypvprA"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B4aDzKS_vqsq",
    "outputId": "130304dc-121c-4dec-8d18-87dc87c177fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18m 17s (- eta: 9343m 47s) (5000 0%) 4.3753\n",
      "31m 20s (- eta: 7986m 26s) (10000 0%) 4.0620\n",
      "44m 54s (- eta: 7613m 39s) (15000 0%) 3.8867\n",
      "58m 2s (- eta: 7365m 32s) (20000 0%) 3.7661\n",
      "71m 20s (- eta: 7229m 28s) (25000 0%) 3.7027\n",
      "83m 49s (- eta: 7064m 16s) (30000 1%) 3.6058\n",
      "95m 43s (- eta: 6900m 32s) (35000 1%) 3.5574\n",
      "107m 28s (- eta: 6765m 44s) (40000 1%) 3.4827\n",
      "119m 23s (- eta: 6668m 5s) (45000 1%) 3.4969\n",
      "131m 38s (- eta: 6603m 48s) (50000 1%) 3.4026\n",
      "144m 3s (- eta: 6556m 34s) (55000 2%) 3.4290\n",
      "156m 26s (- eta: 6513m 53s) (60000 2%) 3.3659\n",
      "168m 45s (- eta: 6473m 1s) (65000 2%) 3.3289\n",
      "181m 7s (- eta: 6437m 55s) (70000 2%) 3.3290\n",
      "193m 20s (- eta: 6401m 25s) (75000 2%) 3.2717\n",
      "205m 38s (- eta: 6370m 15s) (80000 3%) 3.2268\n",
      "217m 56s (- eta: 6341m 18s) (85000 3%) 3.2280\n",
      "230m 17s (- eta: 6315m 36s) (90000 3%) 3.2421\n",
      "242m 39s (- eta: 6291m 50s) (95000 3%) 3.2081\n",
      "254m 59s (- eta: 6268m 18s) (100000 3%) 3.1886\n",
      "267m 25s (- eta: 6248m 0s) (105000 4%) 3.1813\n",
      "279m 46s (- eta: 6226m 49s) (110000 4%) 3.1644\n",
      "292m 11s (- eta: 6207m 29s) (115000 4%) 3.1548\n",
      "304m 38s (- eta: 6189m 41s) (120000 4%) 3.1535\n",
      "317m 1s (- eta: 6171m 6s) (125000 4%) 3.4180\n",
      "329m 26s (- eta: 6153m 27s) (130000 5%) 3.4604\n",
      "341m 48s (- eta: 6135m 15s) (135000 5%) 3.4017\n",
      "354m 13s (- eta: 6118m 31s) (140000 5%) 3.3237\n",
      "366m 34s (- eta: 6100m 36s) (145000 5%) 3.2783\n",
      "378m 55s (- eta: 6083m 20s) (150000 5%) 3.2966\n",
      "391m 23s (- eta: 6068m 11s) (155000 6%) 3.2954\n",
      "403m 48s (- eta: 6052m 36s) (160000 6%) 3.2703\n",
      "416m 12s (- eta: 6036m 43s) (165000 6%) 3.1963\n",
      "428m 33s (- eta: 6020m 25s) (170000 6%) 3.2105\n",
      "440m 55s (- eta: 6004m 39s) (175000 6%) 3.1864\n",
      "453m 20s (- eta: 5989m 33s) (180000 7%) 3.1970\n",
      "465m 45s (- eta: 5974m 40s) (185000 7%) 3.1754\n",
      "478m 2s (- eta: 5958m 17s) (190000 7%) 3.1235\n",
      "490m 32s (- eta: 5944m 51s) (195000 7%) 3.1853\n",
      "502m 57s (- eta: 5930m 20s) (200000 7%) 3.2100\n",
      "515m 19s (- eta: 5915m 24s) (205000 8%) 3.1572\n",
      "527m 45s (- eta: 5901m 15s) (210000 8%) 3.1248\n",
      "540m 12s (- eta: 5887m 25s) (215000 8%) 3.1070\n",
      "552m 39s (- eta: 5873m 39s) (220000 8%) 3.1465\n",
      "565m 4s (- eta: 5859m 38s) (225000 8%) 3.1133\n",
      "577m 33s (- eta: 5846m 19s) (230000 8%) 3.1002\n",
      "590m 3s (- eta: 5833m 13s) (235000 9%) 3.1371\n",
      "602m 27s (- eta: 5819m 10s) (240000 9%) 3.0850\n",
      "614m 52s (- eta: 5805m 22s) (245000 9%) 3.1507\n",
      "627m 13s (- eta: 5790m 58s) (250000 9%) 3.1603\n",
      "639m 37s (- eta: 5777m 3s) (255000 9%) 3.0715\n",
      "651m 58s (- eta: 5762m 48s) (260000 10%) 3.1020\n",
      "665m 22s (- eta: 5757m 45s) (265000 10%) 3.1107\n",
      "678m 1s (- eta: 5746m 2s) (270000 10%) 3.0925\n",
      "690m 29s (- eta: 5732m 49s) (275000 10%) 3.1112\n",
      "703m 2s (- eta: 5720m 11s) (280000 10%) 3.1051\n",
      "715m 43s (- eta: 5708m 40s) (285000 11%) 3.1145\n",
      "728m 17s (- eta: 5696m 10s) (290000 11%) 3.1333\n",
      "740m 51s (- eta: 5683m 44s) (295000 11%) 3.1125\n",
      "753m 28s (- eta: 5671m 31s) (300000 11%) 3.0890\n",
      "766m 7s (- eta: 5659m 44s) (305000 11%) 3.0724\n",
      "778m 46s (- eta: 5647m 46s) (310000 12%) 3.0645\n",
      "791m 26s (- eta: 5635m 59s) (315000 12%) 3.1278\n",
      "803m 54s (- eta: 5622m 47s) (320000 12%) 3.0818\n",
      "816m 19s (- eta: 5609m 11s) (325000 12%) 3.0753\n",
      "828m 51s (- eta: 5596m 28s) (330000 12%) 3.0470\n",
      "841m 27s (- eta: 5584m 14s) (335000 13%) 3.0873\n",
      "854m 4s (- eta: 5571m 58s) (340000 13%) 3.1744\n",
      "866m 33s (- eta: 5558m 59s) (345000 13%) 3.1783\n",
      "879m 6s (- eta: 5546m 17s) (350000 13%) 3.1935\n",
      "891m 47s (- eta: 5534m 32s) (355000 13%) 3.2283\n",
      "904m 19s (- eta: 5521m 49s) (360000 14%) 3.1944\n",
      "916m 57s (- eta: 5509m 40s) (365000 14%) 3.2041\n",
      "929m 30s (- eta: 5497m 4s) (370000 14%) 3.1896\n",
      "942m 7s (- eta: 5484m 50s) (375000 14%) 3.1403\n",
      "954m 43s (- eta: 5472m 31s) (380000 14%) 3.1347\n",
      "967m 20s (- eta: 5460m 15s) (385000 15%) 3.1103\n",
      "979m 54s (- eta: 5447m 44s) (390000 15%) 3.1721\n",
      "992m 26s (- eta: 5434m 56s) (395000 15%) 3.1650\n",
      "1005m 8s (- eta: 5423m 10s) (400000 15%) 3.1765\n",
      "1017m 44s (- eta: 5410m 47s) (405000 15%) 3.1520\n",
      "1030m 23s (- eta: 5398m 38s) (410000 16%) 3.1719\n",
      "1042m 57s (- eta: 5386m 8s) (415000 16%) 3.1464\n",
      "1055m 31s (- eta: 5373m 35s) (420000 16%) 3.1351\n",
      "1068m 4s (- eta: 5360m 53s) (425000 16%) 3.1352\n",
      "1080m 41s (- eta: 5348m 34s) (430000 16%) 3.1694\n",
      "1093m 16s (- eta: 5336m 4s) (435000 17%) 3.1059\n",
      "1105m 52s (- eta: 5323m 41s) (440000 17%) 3.0969\n",
      "1118m 24s (- eta: 5311m 0s) (445000 17%) 3.1065\n",
      "1130m 59s (- eta: 5298m 30s) (450000 17%) 3.1088\n",
      "1143m 32s (- eta: 5285m 51s) (455000 17%) 3.0938\n",
      "1156m 8s (- eta: 5273m 25s) (460000 17%) 3.1169\n",
      "1168m 43s (- eta: 5260m 57s) (465000 18%) 3.0974\n",
      "1181m 26s (- eta: 5249m 2s) (470000 18%) 3.1423\n",
      "1194m 5s (- eta: 5236m 51s) (475000 18%) 3.0988\n",
      "1206m 44s (- eta: 5224m 38s) (480000 18%) 3.0694\n",
      "1219m 17s (- eta: 5211m 56s) (485000 18%) 3.0563\n",
      "1231m 56s (- eta: 5199m 45s) (490000 19%) 3.0622\n",
      "1244m 35s (- eta: 5187m 29s) (495000 19%) 3.0912\n",
      "1257m 11s (- eta: 5175m 1s) (500000 19%) 3.1077\n",
      "1269m 50s (- eta: 5162m 47s) (505000 19%) 3.0719\n",
      "1282m 25s (- eta: 5150m 14s) (510000 19%) 3.1419\n",
      "1295m 10s (- eta: 5138m 24s) (515000 20%) 3.1548\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m encoder1 \u001b[38;5;241m=\u001b[39m EncoderRNN(input_lang\u001b[38;5;241m.\u001b[39mn_words, hidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m decoder1 \u001b[38;5;241m=\u001b[39m DecoderRNN(hidden_size, output_lang\u001b[38;5;241m.\u001b[39mn_words)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrainIters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mtrainIters\u001b[1;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[0;32m     14\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m training_pair[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     15\u001b[0m target_tensor \u001b[38;5;241m=\u001b[39m training_pair[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m print_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     19\u001b[0m plot_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length\u001b[38;5;241m=\u001b[39mMAX_LENGTH):\n\u001b[0;32m      5\u001b[0m     encoder_hidden \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39minitHidden()\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mencoder_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     decoder_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      8\u001b[0m     input_length \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:249\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    247\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m foreach \u001b[38;5;129;01mor\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse):\n\u001b[1;32m--> 249\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     per_device_and_dtype_grads[p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdevice][p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype]\u001b[38;5;241m.\u001b[39mappend(p\u001b[38;5;241m.\u001b[39mgrad)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "trainIters(encoder1, decoder1, 10*len(pairs), print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_j3EeZACeaTg"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=1000):\n",
    "    candidate_corpus = []\n",
    "    references_corpus = []\n",
    "    for i in range(n):\n",
    "        coded, origin = random.choice(pairs)\n",
    "        references_corpus.append(origin.split(' '))\n",
    "        output_words = evaluate(encoder, decoder, coded)\n",
    "        candidate_corpus.append(output_words)\n",
    "        output_sentence = ' '.join(output_words[:-1])\n",
    "        if i%100 == 0:\n",
    "            print('>', coded)\n",
    "            print('=', origin)\n",
    "            print('<', output_sentence, '  --> ', output_sentence.replace(' <EOS>', '') == origin)\n",
    "            print('')\n",
    "\n",
    "    print(f'BLEU {bleu_score(candidate_corpus, references_corpus, max_n=2, weights = [0.5, 0.5])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWPaKZTNvtUt",
    "outputId": "0b78d337-2bba-4d5a-f960-a0af3941c8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> lass mich bitte ausreden !\n",
      "= please let me finish .\n",
      "< please me me me   -->  False\n",
      "\n",
      "> es stort mich nicht meinen tisch zu teilen .\n",
      "= i don t mind sharing my table .\n",
      "< don t to to   -->  False\n",
      "\n",
      "> tom hat ein bisschen zu viel auf den rippen .\n",
      "= tom is a bit on the heavy side .\n",
      "< tom has a lot on to bit good .   -->  False\n",
      "\n",
      "> ich frage ihn morgen danach .\n",
      "= i will ask him about it tomorrow .\n",
      "< i m going to to   -->  False\n",
      "\n",
      "> glaub mir einfach .\n",
      "= just take my word for it .\n",
      "< i believe to   -->  False\n",
      "\n",
      "> wir haben es vergessen .\n",
      "= we forgot .\n",
      "< we were forgotten   -->  False\n",
      "\n",
      "> sei freundlich zu alten menschen .\n",
      "= be kind to the old .\n",
      "< be people some .   -->  False\n",
      "\n",
      "> tom und maria hatten die gleiche idee .\n",
      "= tom and mary had the same idea .\n",
      "< tom and mary and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and   -->  False\n",
      "\n",
      "> versuche einfach ruhig zu bleiben !\n",
      "= just try and stay calm .\n",
      "< try try to   -->  False\n",
      "\n",
      "> tom tragt gerne locker sitzende kleidung .\n",
      "= tom likes to wear loose fitting clothes .\n",
      "< tom likes to   -->  False\n",
      "\n",
      "BLEU 0.008625558577477932\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
